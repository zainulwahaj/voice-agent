# Voice Calendar Assistant - Direct OpenAI Realtime API Implementation

This is a **complete rebuild** of the voice agent using the **OpenAI Realtime API directly** via WebSocket, without relying on the unreliable `@openai/agents-realtime` SDK.

## ğŸ¯ Why the Rebuild?

The `@openai/agents-realtime` v0.1.6 SDK was causing issues:
- Voice input not processing correctly
- Agent would start/stop immediately without capturing speech
- No transcription events firing
- Unreliable for production use

## âœ¨ New Architecture

### Direct WebSocket Connection
- **No SDK dependencies** - we connect directly to OpenAI's Realtime API
- Full control over audio streaming, event handling, and tool calling
- More reliable and predictable behavior

### Key Components

1. **`realtime-api.ts`** - Core WebSocket client
   - Handles WebSocket connection to `wss://api.openai.com/v1/realtime`
   - Manages audio capture from microphone (PCM16 format)
   - Handles audio playback from assistant
   - Event system for all Realtime API events

2. **`main-realtime.ts`** - Application logic
   - Fetches MCP tools from calendar server
   - Registers tools with OpenAI session
   - Handles tool calls (function calling)
   - Calls MCP server and returns results
   - Updates UI with conversation flow

3. **`index-new.html`** - Clean UI
   - Initialize agent
   - Start/stop conversation
   - Test MCP connection
   - View live conversation transcript

## ğŸš€ Setup

### 1. Install Dependencies

```bash
npm install
```

**Note:** We've removed `@openai/agents-realtime` from package.json

### 2. Set Environment Variables

Create `.env` file:

```bash
VITE_OPENAI_API_KEY=sk-proj-...
```

### 3. Start MCP Server

The Google Calendar MCP server must be running on port 3000:

```bash
cd ../google-calendar-mcp
npm run dev:http
```

### 4. Start Voice Agent

```bash
npm run dev
```

Then open `http://localhost:5173/index-new.html`

## ğŸ“‹ How It Works

### Initialization Flow

1. **Fetch MCP Tools**
   ```typescript
   GET http://localhost:3000/mcp/tools
   â†’ Returns list of 10 calendar tools with schemas
   ```

2. **Connect to OpenAI**
   ```typescript
   WebSocket â†’ wss://api.openai.com/v1/realtime
   Authentication via WebSocket subprotocol
   ```

3. **Configure Session**
   ```typescript
   send({
     type: 'session.update',
     session: {
       tools: [...mcpTools],  // Register calendar tools
       turn_detection: 'server_vad',
       voice: 'alloy'
     }
   })
   ```

4. **Start Audio Capture**
   ```typescript
   navigator.mediaDevices.getUserMedia({ audio: true })
   â†’ Capture PCM16 audio at 24kHz
   â†’ Send via input_audio_buffer.append
   ```

### Conversation Flow

1. **User speaks** â†’ Microphone captures audio
2. **Audio sent to OpenAI** â†’ Transcription + processing
3. **OpenAI decides** â†’ "User wants calendar info"
4. **Function call** â†’ `response.function_call_arguments.done`
5. **Call MCP server** â†’ Execute `list-events` tool
6. **Return result** â†’ `conversation.item.create` with output
7. **OpenAI responds** â†’ Natural language + audio
8. **Audio played** â†’ User hears assistant's response

### Tool Calling Example

```typescript
// OpenAI decides to call list-events
Event: response.function_call_arguments.done
{
  call_id: "abc123",
  name: "list-events",
  arguments: '{"calendarId":"primary","maxResults":10}'
}

// We call MCP server
POST http://localhost:3000/mcp/call-tool
{
  method: "tools/call",
  params: {
    name: "list-events",
    arguments: { calendarId: "primary", maxResults: 10 }
  }
}

// Return result to OpenAI
send({
  type: 'conversation.item.create',
  item: {
    type: 'function_call_output',
    call_id: 'abc123',
    output: JSON.stringify(calendarEvents)
  }
})

// Request assistant to respond
send({ type: 'response.create' })
```

## ğŸ¤ Audio Pipeline

### Input (Microphone â†’ OpenAI)

1. Capture audio at 24kHz mono
2. Convert Float32 â†’ PCM16
3. Encode to base64
4. Send via `input_audio_buffer.append`

```typescript
const pcm16 = float32ToPCM16(audioData)
const base64 = arrayBufferToBase64(pcm16)

send({
  type: 'input_audio_buffer.append',
  audio: base64
})
```

### Output (OpenAI â†’ Speaker)

1. Receive `response.audio.delta` events
2. Decode base64 â†’ PCM16
3. Convert PCM16 â†’ Float32
4. Create AudioBuffer
5. Play via Web Audio API

```typescript
on('response.audio.delta', (event) => {
  const pcm16 = base64ToArrayBuffer(event.delta)
  const float32 = pcm16ToFloat32(pcm16)
  const audioBuffer = audioContext.createBuffer(...)
  playAudio(audioBuffer)
})
```

## ğŸ› ï¸ Available Tools

The agent has access to 10 Google Calendar tools:

1. **list-calendars** - Get all calendars
2. **list-events** - List events in date range
3. **search-events** - Search events by query
4. **get-event** - Get specific event details
5. **create-event** - Create new event
6. **update-event** - Update existing event
7. **delete-event** - Delete event
8. **get-freebusy** - Check free/busy times
9. **list-colors** - Get available colors
10. **get-current-time** - Get current time

## ğŸ§ª Testing

### Test MCP Connection
Click "Test MCP Connection" button to verify:
- MCP server is running on port 3000
- Tools are being returned correctly
- HTTP transport is working

### Test List Events
Click "Test List Events" to verify:
- Can call MCP tools directly
- Calendar data is accessible
- OAuth authentication is working

### Voice Testing
1. Click "Initialize Agent"
2. Click "Start Conversation"
3. Grant microphone permission
4. Say: "What's on my calendar today?"
5. Wait for assistant response

## ğŸ› Debugging

### Check Console Logs

```
ğŸ”Œ Connected to OpenAI Realtime API
âœ… Session created
ğŸ¤ Microphone capture started
ğŸ¤ User started speaking
ğŸ“ User said: "what's on my calendar today"
ğŸ”§ Function call complete: list-events
âœ… MCP tool result: {...}
ğŸ’¬ Assistant said: "You have 3 events today..."
ğŸ”Š Audio response complete
```

### Common Issues

**"WebSocket connection failed"**
- Check VITE_OPENAI_API_KEY is set correctly
- Verify API key has Realtime API access

**"MCP tools fetch failed"**
- Ensure MCP server is running: `npm run dev:http`
- Check http://localhost:3000/mcp/tools responds

**"Microphone not working"**
- Grant microphone permissions in browser
- Check browser console for getUserMedia errors
- Try HTTPS (required for mic access on some browsers)

## ğŸ“Š Event Flow Diagram

```
User speaks
    â†“
Microphone capture (Web Audio API)
    â†“
Float32 â†’ PCM16 â†’ Base64
    â†“
WebSocket: input_audio_buffer.append
    â†“
OpenAI Realtime API
    â†“
Server VAD detects speech end
    â†“
Transcription (Whisper)
    â†“
GPT-4o processes request
    â†“
Decides to call function
    â†“
Event: response.function_call_arguments.done
    â†“
Our handler calls MCP server
    â†“
POST /mcp/call-tool
    â†“
Google Calendar MCP executes
    â†“
Returns calendar data
    â†“
WebSocket: conversation.item.create (result)
    â†“
WebSocket: response.create
    â†“
GPT-4o generates natural response
    â†“
Event: response.audio.delta (base64 audio)
    â†“
Base64 â†’ PCM16 â†’ Float32
    â†“
Web Audio API playback
    â†“
User hears response
```

## ğŸ”„ Differences from SDK Version

### Old (SDK-based)
```typescript
import { RealtimeAgent, RealtimeSession } from '@openai/agents-realtime'

const agent = new RealtimeAgent(config)
await agent.updateTurnDetectionMode('server_vad')
await session.connect()
```

### New (Direct WebSocket)
```typescript
import { RealtimeAPIClient } from './realtime-api'

const client = new RealtimeAPIClient(config)
await client.connect()
client.send({
  type: 'session.update',
  session: { turn_detection: { type: 'server_vad' } }
})
```

**Benefits:**
- âœ… Full control over connection lifecycle
- âœ… Direct access to all Realtime API events
- âœ… No black box - we see exactly what's happening
- âœ… Easier to debug and extend
- âœ… No dependency on potentially buggy SDK

## ğŸ“ Next Steps

1. **Test end-to-end** - Try all calendar operations
2. **Error handling** - Add retry logic for network failures
3. **UI improvements** - Better visual feedback
4. **Conversation history** - Persist chat history
5. **Multi-turn conversations** - Handle complex queries

## ğŸ‰ Success Criteria

- âœ… WebSocket connection established
- âœ… 10 MCP tools fetched and registered
- âœ… Audio capture working
- âœ… Audio playback working
- âœ… Speech detection (VAD) working
- âœ… Transcription events firing
- âœ… Function calling working
- âœ… MCP tool execution working
- âœ… Natural voice responses playing

## ğŸ“š References

- [OpenAI Realtime API Docs](https://platform.openai.com/docs/guides/realtime)
- [Model Context Protocol](https://modelcontextprotocol.io/)
- [Web Audio API](https://developer.mozilla.org/en-US/docs/Web/API/Web_Audio_API)
- [WebSocket API](https://developer.mozilla.org/en-US/docs/Web/API/WebSocket)
